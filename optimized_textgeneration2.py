# -*- coding: utf-8 -*-
"""Optimized_Textgeneration2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h_7hpeTpS3WLu6D91wPtqQowY88FjL5x
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Step 1: Create a Toy Dataset
text = """
Once upon a time, in a small village, there was a kind and brave knight.
The knight loved adventures and helping people. One day, the knight found
a mysterious map that led to hidden treasures in a faraway land.
"""

# Step 2: Tokenize the Text
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
sequences = tokenizer.texts_to_sequences([text])[0]

# Vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary Size: {vocab_size}")

# Step 3: Prepare Input and Output Sequences
sequence_length = 5  # Number of words in each input sequence
X = []
y = []

for i in range(sequence_length, len(sequences)):
    X.append(sequences[i-sequence_length:i])  # Input sequence
    y.append(sequences[i])  # Output word

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Step 4: Build the Optimized Model
model = Sequential([
    Embedding(vocab_size, 10, input_length=sequence_length),  # Embedding layer
    Bidirectional(GRU(128, return_sequences=False)),  # GRU with Bidirectional layer
    Dropout(0.3),
    Dense(vocab_size, activation='softmax')  # Output layer
])

# Compile the model with Adam optimizer and a smaller learning rate
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0005), metrics=['accuracy'])
print("Model Compiled!")

# Step 5: Optimize Training with Early Stopping and Learning Rate Scheduler
early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)

# Train the model
history = model.fit(X, y, epochs=50, batch_size=16, callbacks=[early_stop, lr_scheduler], verbose=2)

# Step 6: Plot Loss and Accuracy
plt.figure(figsize=(12, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Step 7: Generate Text with Optimized Sampling (Temperature & Top-K Sampling)
def generate_text_optimized(seed_text, num_words=50, temperature=1.0, top_k=40):
    """
    Generate text using temperature and top-k sampling.
    """
    for _ in range(num_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')

        # Get the model's predictions
        predictions = model.predict(token_list, verbose=0)[0]

        # Apply temperature to the predictions to control randomness
        predictions = np.log(predictions + 1e-8) / temperature
        predictions = np.exp(predictions) / np.sum(np.exp(predictions))

        # Top-k sampling
        top_indices = predictions.argsort()[-top_k:][::-1]
        top_probs = predictions[top_indices]
        top_probs /= top_probs.sum()

        predicted_index = np.random.choice(top_indices, p=top_probs)
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        seed_text += ' ' + output_word

    return seed_text

# Step 8: Test Optimized Text Generation
seed_text = "The knight embarked on"
generated_text = generate_text_optimized(seed_text, num_words=50, temperature=0.8, top_k=30)
print("\nGenerated Text:")
print(generated_text)

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Step 1: Create a Toy Dataset
text = """
Once upon a time, in a small village, there was a kind and brave knight.
The knight loved adventures and helping people. One day, the knight found
a mysterious map that led to hidden treasures in a faraway land.
"""

# Step 2: Tokenize the Text
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
sequences = tokenizer.texts_to_sequences([text])[0]

# Vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary Size: {vocab_size}")

# Step 3: Prepare Input and Output Sequences
sequence_length = 5  # Number of words in each input sequence
X = []
y = []

for i in range(sequence_length, len(sequences)):
    X.append(sequences[i-sequence_length:i])  # Input sequence
    y.append(sequences[i])  # Output word

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Step 4: Build the Optimized Model with More Complexity
model = Sequential([
    Embedding(vocab_size, 20, input_length=sequence_length),  # Increase embedding size
    Bidirectional(LSTM(128, return_sequences=False)),  # Bidirectional LSTM with 128 units
    Dropout(0.3),  # Add more dropout to prevent overfitting
    Dense(vocab_size, activation='softmax')  # Output layer
])

# Compile the model with Adam optimizer and a smaller learning rate
model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
print("Model Compiled!")

# Step 5: Optimize Training with Early Stopping and Learning Rate Scheduler
early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)

# Train the model
history = model.fit(X, y, epochs=200, batch_size=16, callbacks=[early_stop, lr_scheduler], verbose=2)

# Step 6: Plot Loss and Accuracy
plt.figure(figsize=(12, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Step 7: Generate Text with Optimized Sampling (Temperature & Top-K)
def generate_text_optimized(seed_text, num_words=50, temperature=0.8, top_k=30):
    """
    Generate text using temperature and top-k sampling.
    """
    for _ in range(num_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')

        # Get the model's predictions
        predictions = model.predict(token_list, verbose=0)[0]

        # Apply temperature to the predictions to control randomness
        predictions = np.log(predictions + 1e-8) / temperature
        predictions = np.exp(predictions) / np.sum(np.exp(predictions))

        # Top-k sampling
        top_indices = predictions.argsort()[-top_k:][::-1]
        top_probs = predictions[top_indices]
        top_probs /= top_probs.sum()

        # Randomly pick from the top-k choices
        predicted_index = np.random.choice(top_indices, p=top_probs)

        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break

        seed_text += ' ' + output_word

    return seed_text

# Step 8: Test Optimized Text Generation
seed_text = "The knight embarked on"
generated_text = generate_text_optimized(seed_text, num_words=50, temperature=0.7, top_k=20)
print("\nGenerated Text:")
print(generated_text)