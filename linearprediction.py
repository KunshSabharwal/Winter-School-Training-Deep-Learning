# -*- coding: utf-8 -*-
"""Linearprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TGwavAA5qYJQPDHteXOkRvqbucz1iWr4
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD, Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Generate synthetic linear data
def generate_linear_data(samples=1000):
    X = np.linspace(0, 10, samples).reshape(-1, 1)
    y = 3 * X + np.random.normal(0, 0.5, X.shape)  # y = 3x + noise
    return X, y

# Scale the data
def scale_data(X, y):
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    return X_scaled, y_scaled, scaler_X, scaler_y

# Create a simple neural network model
def create_model():
    model = Sequential([
        Dense(10, activation='relu', input_dim=1),
        Dense(1)
    ])
    return model

# Train and evaluate the model
def train_and_evaluate(optimizer, batch_size, X_train, y_train, X_test, y_test, name):
    model = create_model()
    model.compile(optimizer=optimizer, loss='mse')
    model.fit(X_train, y_train, batch_size=batch_size, epochs=50, verbose=0)
    loss = model.evaluate(X_test, y_test, verbose=0)
    predictions = model.predict(X_test)
    print(f"{name} Test Loss: {loss}")
    return predictions, loss

# Generate data
X, y = generate_linear_data()
X_scaled, y_scaled, scaler_X, scaler_y = scale_data(X, y)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Train using BGD, SGD, and MBGD
print("Training with Batch Gradient Descent (BGD)")
y_pred_bgd, loss_bgd = train_and_evaluate(Adam(learning_rate=0.01), len(X_train), X_train, y_train, X_test, y_test, "BGD")

print("\nTraining with Stochastic Gradient Descent (SGD)")
y_pred_sgd, loss_sgd = train_and_evaluate(SGD(learning_rate=0.01), 1, X_train, y_train, X_test, y_test, "SGD")

print("\nTraining with Mini-Batch Gradient Descent (MBGD)")
y_pred_mbgd, loss_mbgd = train_and_evaluate(Adam(learning_rate=0.01), 32, X_train, y_train, X_test, y_test, "MBGD")

# Plot actual vs predicted
plt.figure(figsize=(10, 6))
plt.scatter(scaler_X.inverse_transform(X_test), scaler_y.inverse_transform(y_test), label="Actual", color="blue")
plt.scatter(scaler_X.inverse_transform(X_test), scaler_y.inverse_transform(y_pred_bgd), label="BGD", color="orange")
plt.scatter(scaler_X.inverse_transform(X_test), scaler_y.inverse_transform(y_pred_sgd), label="SGD", color="green")
plt.scatter(scaler_X.inverse_transform(X_test), scaler_y.inverse_transform(y_pred_mbgd), label="MBGD", color="red")
plt.title("Actual vs Predicted")
plt.xlabel("Input Feature")
plt.ylabel("Target")
plt.legend()
plt.grid()
plt.show()

# Compare losses
losses = [loss_bgd, loss_sgd, loss_mbgd]
labels = ["BGD", "SGD", "MBGD"]
plt.figure(figsize=(8, 5))
plt.bar(labels, losses, color=['orange', 'green', 'red'])
plt.title("Comparison of Gradient Descent Methods")
plt.ylabel("Test Loss")
plt.xlabel("Method")
for i, loss in enumerate(losses):
    plt.text(i, loss + 0.01, f"{loss:.4f}", ha='center')
plt.grid(axis='y')
plt.show()