# -*- coding: utf-8 -*-
"""textgeneration1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-pUKPoCZUbFfNh-p9IaHDjUuCgUbMZ-
"""

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Step 1: Create a Toy Dataset
text = """
Once upon a time, in a small village, there was a kind and brave knight.
The knight loved adventures and helping people. One day, the knight found
a mysterious map that led to hidden treasures in a faraway land.
"""

# Step 2: Tokenize the Text
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
sequences = tokenizer.texts_to_sequences([text])[0]

# Vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary Size: {vocab_size}")

# Step 3: Prepare Input and Output Sequences
sequence_length = 5  # Number of words in each input sequence
X = []
y = []

for i in range(sequence_length, len(sequences)):
    X.append(sequences[i-sequence_length:i])  # Input sequence
    y.append(sequences[i])  # Output word

# Convert to NumPy arrays
X = np.array(X)
y = np.array(y)

# Step 4: Build the RNN Model
model = Sequential([
    Embedding(vocab_size, 10, input_length=sequence_length),  # Embedding layer
    LSTM(50),  # LSTM layer
    Dense(vocab_size, activation='softmax')  # Output layer
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Step 5: Train the Model
model.fit(X, y, epochs=200, verbose=2, batch_size=16)  # Train the model

# Step 6: Generate Text
def generate_text(seed_text, num_words):
    for _ in range(num_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')
        predicted_index = np.argmax(model.predict(token_list, verbose=0))
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

#Step 7: Test the Text Generation
seed_text = "Once upon a time"
num_words_to_generate = 20
generated_text = generate_text(seed_text, num_words_to_generate)

print("\nGenerated Text:")
print(generated_text)

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model

# Step 1: Save the Trained Model
model.save('text_gen_model.h5')  # Save the model
print("Model saved successfully!")

# Step 2: Reload the Model
loaded_model = load_model('text_gen_model.h5')
print("Model loaded successfully!")

# Step 3: Text Generation Function
def generate_text_from_loaded_model(seed_text, num_words):
    """
    Generate text using the loaded model.
    """
    for _ in range(num_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')
        predicted_index = np.argmax(loaded_model.predict(token_list, verbose=0))
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Step 4: Generate Text Using the Reloaded Model
seed_text = "Once upon a time"
num_words_to_generate = 30  # Generate 30 words

# Generate text
generated_text = generate_text_from_loaded_model(seed_text, num_words_to_generate)
print("\nGenerated Text from Saved Model:")
print(generated_text)

"""**Code for Training Visualization and Fine-Tuning**
This will help you understand how the model performs over epochs and fine-tune the generated text for better results.

Training History:

Uses the History callback to record loss and accuracy during training.
Plots graphs to visualize how well the model is learning.
Fine-Tuning with Temperature:

Temperature controls randomness in predictions:
Low Temperature (0.5): Predictable and repetitive text.
High Temperature (1.5): Creative and diverse text, but may be less coherent.
Text Sampling:

Instead of always picking the most probable word, this code introduces randomness by sampling words based on probabilities.
Steps to Run
Run the code after training the model.
Visualize training performance through loss and accuracy graphs.
Experiment with different temperature values in text generation to find the best balance between coherence and creativity.
Why This is Important
Understand and debug training performance.
Generate more interesting and realistic text by adjusting randomness (temperature).
Fine-tune the generation process to match your desired output style.
"""

import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import History

# Step 1: Retrain the Model with Training History
history = History()
model.fit(X, y, epochs=50, batch_size=32, callbacks=[history], verbose=2)

# Step 2: Plot Loss and Accuracy
plt.figure(figsize=(12, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy', color='green')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Step 3: Fine-Tune Text Generation
def generate_fine_tuned_text(seed_text, num_words, temperature=1.0):
    """
    Generates text using the model with temperature-based sampling.

    Parameters:
    - seed_text: Starting text.
    - num_words: Number of words to generate.
    - temperature: Controls randomness in prediction. Lower = deterministic, Higher = more random.
    """
    for _ in range(num_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=sequence_length, padding='pre')
        predictions = model.predict(token_list, verbose=0).flatten()

        # Apply temperature to predictions
        predictions = np.log(predictions + 1e-8) / temperature
        predictions = np.exp(predictions) / np.sum(np.exp(predictions))

        # Sample the next word index
        predicted_index = np.random.choice(range(len(predictions)), p=predictions)

        # Find the corresponding word
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Step 4: Test Fine-Tuned Text Generation
seed_text = "Once upon a time"
num_words_to_generate = 30

# Experiment with different temperatures
for temp in [0.5, 1.0, 1.5]:
    print(f"\nGenerated Text with Temperature {temp}:")
    print(generate_fine_tuned_text(seed_text, num_words_to_generate, temperature=temp))

"""**Code for Saving History and Multiple Seed **"""

import json
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Step 1: Save the Training History
history_dict = {
    'loss': history.history['loss'],
    'accuracy': history.history['accuracy'],
}

with open('training_history.json', 'w') as f:
    json.dump(history_dict, f)
print("Training history saved as 'training_history.json'.")

# Step 2: Load the Saved Model
model = load_model('text_gen_model.h5')  # Make sure the model is saved as 'text_gen_model.h5'

# Step 3: Test Text Generation with Multiple Seed Texts
def generate_text_with_multiple_seeds(seed_texts, num_words_to_generate=30):
    for seed_text in seed_texts:
        print(f"\nSeed Text: {seed_text}")
        generated_text = generate_text_from_loaded_model(seed_text, num_words_to_generate)
        print("Generated Text:")
        print(generated_text)

# Test with different seed texts
seed_texts = [
    "Once upon a time",
    "In the distant future",
    "The knight embarked on",
    "A long time ago",
    "The adventure began when"
]

# Run the test
generate_text_with_multiple_seeds(seed_texts, num_words_to_generate=30)