# -*- coding: utf-8 -*-
"""optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dI-jocYbs5KRvZAfSa7aUTxsnh0b2R30
"""

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import SGD, Adam

# Generate synthetic time-series data
def generate_data(timesteps=10, samples=1000):
    X = np.random.randn(samples, timesteps, 1)  # Random time-series data
    y = np.random.randn(samples, 1)  # Random labels
    return X, y

# Generate training and testing data
X_train, y_train = generate_data(samples=800)
X_test, y_test = generate_data(samples=200)

# Define an LSTM model
def create_model():
    model = Sequential([
        LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
        Dense(1)
    ])
    return model

# Training with Batch Gradient Descent (BGD)
print("Training with Batch Gradient Descent (BGD)")
model_bgd = create_model()
model_bgd.compile(optimizer=Adam(), loss='mse')
history_bgd = model_bgd.fit(X_train, y_train, batch_size=len(X_train), epochs=10, verbose=1, validation_data=(X_test, y_test))

# Training with Stochastic Gradient Descent (SGD)
print("\nTraining with Stochastic Gradient Descent (SGD)")
model_sgd = create_model()
model_sgd.compile(optimizer=SGD(learning_rate=0.01), loss='mse')
history_sgd = model_sgd.fit(X_train, y_train, batch_size=1, epochs=10, verbose=1, validation_data=(X_test, y_test))

# Training with Mini-Batch Gradient Descent (MBGD)
print("\nTraining with Mini-Batch Gradient Descent (MBGD)")
model_mbgd = create_model()
model_mbgd.compile(optimizer=Adam(), loss='mse')
history_mbgd = model_mbgd.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))

# Evaluate models
def evaluate_model(model, name):
    loss = model.evaluate(X_test, y_test, verbose=0)
    print(f"{name} Test Loss: {loss}")

evaluate_model(model_bgd, "BGD")
evaluate_model(model_sgd, "SGD")
evaluate_model(model_mbgd, "MBGD")

# Visualization of Loss Curves
def plot_loss_curves(histories, labels):
    plt.figure(figsize=(12, 6))
    for history, label in zip(histories, labels):
        plt.plot(history.history['loss'], label=f"{label} Train Loss")
        plt.plot(history.history['val_loss'], label=f"{label} Validation Loss")
    plt.title("Loss Curves for Different Optimization Techniques")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)
    plt.show()

# Plot the loss curves
plot_loss_curves(
    histories=[history_bgd, history_sgd, history_mbgd],
    labels=["BGD", "SGD", "MBGD"]
)